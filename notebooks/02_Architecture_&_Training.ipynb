{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd02923c",
   "metadata": {},
   "source": [
    "# GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57a2d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd65a1a0",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding\n",
    "\n",
    "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
    "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- In this chapter, we are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library, which implements its core algorithms in Rust to improve computational performance\n",
    "- I created a notebook in the [./bytepair_encoder](../02_bonus_bytepair-encoder) that compares these two implementations side-by-side (tiktoken was about 5x faster on the sample text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6db6c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bf398",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Data Sampling with a sliding window\n",
    "\n",
    "- We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb77b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75bc6599",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Token Embeddings\n",
    "\n",
    "- Let us embed the tokens in a continuous vector representation using an embedding layer\n",
    "- Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc2f473f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(6, 3)\n",
      "Weight Matrix:\n",
      " Parameter containing:\n",
      "tensor([[ 3.3737e-01, -1.7778e-01, -1.6896e-01],\n",
      "        [ 9.1776e-01,  1.5810e+00,  1.3010e+00],\n",
      "        [ 1.2753e+00, -2.0095e-01, -1.6056e-01],\n",
      "        [-4.0149e-01,  9.6657e-01, -1.1481e+00],\n",
      "        [-1.1589e+00,  3.2547e-01, -6.3151e-01],\n",
      "        [-2.8400e+00, -7.8485e-01, -1.4096e+00]], requires_grad=True)\n",
      "Token with id 3:\n",
      " tensor([[-4.0149e-01,  9.6657e-01, -1.1481e+00]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 1.2753e+00, -2.0095e-01, -1.6056e-01],\n",
      "        [-4.0149e-01,  9.6657e-01, -1.1481e+00],\n",
      "        [-2.8400e+00, -7.8485e-01, -1.4096e+00],\n",
      "        [ 9.1776e-01,  1.5810e+00,  1.3010e+00]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer)\n",
    "print(\"Weight Matrix:\\n\", embedding_layer.weight)\n",
    "print(\"Token with id 3:\\n\", embedding_layer(torch.tensor([3])))\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619040f0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Encoding Word Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ded5181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      " torch.Size([4, 3])\n",
      "tensor([[-6.3068e-01,  1.2340e+00,  3.1268e-01],\n",
      "        [ 6.9719e-01, -9.9505e-01, -1.1476e+00],\n",
      "        [-9.1777e-01,  9.0452e-01, -2.0975e+00],\n",
      "        [ 1.1558e+00, -1.2157e+00,  1.2952e-01]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 6.4463e-01,  1.0331e+00,  1.5211e-01],\n",
      "        [ 2.9570e-01, -2.8477e-02, -2.2958e+00],\n",
      "        [-3.7578e+00,  1.1966e-01, -3.5071e+00],\n",
      "        [ 2.0735e+00,  3.6532e-01,  1.4306e+00]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_length=4\n",
    "output_dim = 3\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(\"Shape:\\n\", pos_embeddings.shape)\n",
    "print(pos_embeddings)\n",
    "\n",
    "input_embeddings = embedding_layer(input_ids) + pos_embeddings\n",
    "print(input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c64013",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Masked Multi Head Attention (with weight splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c1ed501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 1.5692e-01, -8.7313e-02,  2.1006e-02,  2.1526e-02, -3.2431e-01,\n",
      "          -2.5176e-01],\n",
      "         [ 1.1170e-01, -5.4734e-02,  4.0633e-02, -2.1268e-02, -3.2512e-01,\n",
      "          -2.9932e-01],\n",
      "         [ 1.1956e-01, -4.9084e-02,  3.1778e-02, -6.3516e-02, -2.7884e-01,\n",
      "          -2.5779e-01]],\n",
      "\n",
      "        [[ 1.5692e-01, -8.7313e-02,  2.1006e-02,  2.1526e-02, -3.2431e-01,\n",
      "          -2.5176e-01],\n",
      "         [ 1.1170e-01, -5.4734e-02,  4.0633e-02, -2.1268e-02, -3.2512e-01,\n",
      "          -2.9932e-01],\n",
      "         [ 1.1956e-01, -4.9084e-02,  3.1778e-02, -6.3516e-02, -2.7884e-01,\n",
      "          -2.5779e-01]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out # (6)\n",
    "        self.num_heads = num_heads # (2)\n",
    "        self.head_dim = d_out // num_heads # (3)\n",
    "\n",
    "        # Weight Metrices\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) # (6, 6)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias) # (6, 6)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) # (6, 6)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # (1,3,6)\n",
    "\n",
    "        # query, key and value metrices\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)  (1,3,6)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # unroll\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)   # (1,3,2,3)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now group metrices by number of heads, instead of number of tokens\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1,2)  # (1,2,3,3)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)  # (1,2,3,3)  # (b, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # (1,3,2,3)\n",
    "        \n",
    "        # Combine heads (flatten), where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)  # (1,3,6)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943d54f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e7c6ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6669,  0.5074, -1.1026, -0.3533, -1.7799],\n",
      "        [ 0.6474,  0.5460,  0.8050, -1.3467, -0.6418]])\n",
      "\n",
      "\n",
      "\n",
      "Mean and Variance of batch: tensor(-0.3385) tensor(0.8506)\n",
      "\n",
      "\n",
      "\n",
      "tensor([[ 0.0160,  1.5565, -0.5556,  0.4273, -1.4442],\n",
      "        [ 0.7616,  0.6419,  0.9476, -1.5915, -0.7597]], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Mean and Variance of Normalized Layer: tensor([[-0.0000],\n",
      "        [-0.0000]], grad_fn=<MeanBackward1>) tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x + self.shift\n",
    "\n",
    "batch_example = torch.randn(2, 5)\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "\n",
    "torch.set_printoptions(sci_mode = False)\n",
    "print(batch_example)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Mean and Variance of batch:\", batch_example.mean(), batch_example.var())\n",
    "print(\"\\n\\n\")\n",
    "print(out_ln)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Mean and Variance of Normalized Layer:\", out_ln.mean(dim=-1, keepdim=True), out_ln.var(dim=-1, keepdim=True, unbiased=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e4079",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Feed Forward Neural Network with GELU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cfbfc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# GELU with Approximation\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1+torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0/torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "ffnn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffnn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7877e0e0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Shortcut Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5583dc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n",
      "\n",
      "\n",
      "\n",
      "layers.0.0.weight has gradient mean of 0.22169798612594604\n",
      "layers.1.0.weight has gradient mean of 0.20694111287593842\n",
      "layers.2.0.weight has gradient mean of 0.3289700150489807\n",
      "layers.3.0.weight has gradient mean of 0.26657330989837646\n",
      "layers.4.0.weight has gradient mean of 1.3258544206619263\n"
     ]
    }
   ],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f'{name} has gradient mean of {param.grad.abs().mean().item()}')\n",
    "\n",
    "\n",
    "\n",
    "layer_sizes = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut = False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be44201e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2764c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([2, 4, 768])\n",
      "Output Shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input Shape:\", x.shape)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e99fe17",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44532cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output Shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input Batch:\\n\", batch)\n",
    "print(\"\\nOutput Shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "beb4d5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n",
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n",
      "Number of trainable parameters considering weight tying: 124,412,160\n",
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n",
    "\n",
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb69e1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5eda92f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n",
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "out =  generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90343259",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/15.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096c09c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fdf3e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Samoa parad Defensive MacBook Referospace preparation Einstein ShepherdMot\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads  \n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded  = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d92565a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f55b1c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[ 5873],\n",
      "         [42899],\n",
      "         [26729]],\n",
      "\n",
      "        [[ 9580],\n",
      "         [24016],\n",
      "         [16637]]])\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  percentage Skinner Theme\n",
      "Text 1: tensor([4.0363e-05, 2.7361e-05, 1.1445e-05])\n",
      "Text 2: tensor([8.2989e-06, 9.1948e-05, 6.1120e-06])\n",
      "tensor([-1.0118e+01, -1.0506e+01, -1.1378e+01, -1.1699e+01, -9.2943e+00,\n",
      "        -1.2005e+01])\n",
      "tensor(-1.0833e+01)\n",
      "tensor(1.0833e+01)\n",
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n",
      "tensor(1.0833e+01)\n",
      "tensor(5.0689e+04)\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)\n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "\n",
    "torch.set_printoptions(sci_mode = True)\n",
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)\n",
    "\n",
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)\n",
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)\n",
    "\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)\n",
    "\n",
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)\n",
    "\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a35db03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    text_data = response.text\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34754b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1 : i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                        stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be46237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio*len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23724ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")\n",
    "\n",
    "\n",
    "print(\"Train Loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9d88b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss=0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11b17d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n",
      "Training loss: 10.985983424716526\n",
      "Validation loss: 10.993607521057129\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Use PyTorch 2.9 or newer for stable mps results\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51ac3a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "190758e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                        eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # calculate loss gradients\n",
    "            optimizer.step()  # update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                        f\"Train loss {train_loss:.3f}, val loss {val_loss:.3f}\")\n",
    "        \n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e847811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.821, val loss 9.931\n",
      "Ep 1 (Step 000005): Train loss 8.069, val loss 8.334\n",
      "Every effor moves you,,,,,,,,,,,,,.                                    \n",
      "Ep 2 (Step 000010): Train loss 6.623, val loss 7.049\n",
      "Ep 2 (Step 000015): Train loss 6.046, val loss 6.598\n",
      "Every effor moves you,,, and,,,,,,,,,.                                    \n",
      "Ep 3 (Step 000020): Train loss 5.558, val loss 6.503\n",
      "Ep 3 (Step 000025): Train loss 5.471, val loss 6.392\n",
      "Every effor moves you.                                                 \n",
      "Ep 4 (Step 000030): Train loss 4.995, val loss 6.275\n",
      "Ep 4 (Step 000035): Train loss 4.756, val loss 6.289\n",
      "Every effor moves you.      \"I had been the the of the of the picture--and--and     \"I was a the the picture.               \n",
      "Ep 5 (Step 000040): Train loss 4.113, val loss 6.182\n",
      "Every effor moves you know, and in the picture of the \"I had been. \"Oh, I felt--and it's the  \"Oh, and I had been the donkey--and it to me, and down the \"Oh, and\n",
      "Ep 6 (Step 000045): Train loss 3.721, val loss 6.143\n",
      "Ep 6 (Step 000050): Train loss 3.170, val loss 6.147\n",
      "Every effor moves you know, and in the picture.  \"I had the last word.     \"I didn't. \"Oh, and I had a little. \"I had a little a--I turned, I had been\n",
      "Ep 7 (Step 000055): Train loss 3.109, val loss 6.186\n",
      "Ep 7 (Step 000060): Train loss 2.370, val loss 6.126\n",
      "Every effor moves you know, and in the picture.  \"I had the last word.     \"I didn't you know, I was his pictures--and I was his pictures I had the donkey, and I was a little his pictures\n",
      "Ep 8 (Step 000065): Train loss 1.911, val loss 6.151\n",
      "Ep 8 (Step 000070): Train loss 1.592, val loss 6.223\n",
      "Every effor moves you know; and in the picture. Gisburn--as such--had not till I had been me.  \"Oh, in the moment--as Jack himself, as he was his own sex, and down the room, in his own\n",
      "Ep 9 (Step 000075): Train loss 1.239, val loss 6.259\n",
      "Ep 9 (Step 000080): Train loss 0.957, val loss 6.270\n",
      "Every effor moves you know.\"   I glanced after him, and uncertain.  \"Once, I was, my dear, and that, my dear, his close grayish beard--as if he had the donkey. \"There were days when I couldn\n",
      "Ep 10 (Step 000085): Train loss 0.708, val loss 6.387\n",
      "Every effor moves you know.\" \"I felt able to face the fact with a laugh: \"Yes--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I couldn\n",
      "Training completed in 0.48 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effor moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18f9faf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6e6bb3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Decoding Strategies to control randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118a059",
   "metadata": {},
   "source": [
    "## 1. Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e6597fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effor moves you know.\"\n",
      "\"I felt able to face the fact with a laugh: \"Yes--and by me!\"\n",
      "\n",
      "He\n"
     ]
    }
   ],
   "source": [
    "inference_device = torch.device('cuda')\n",
    "\n",
    "model.to(inference_device)\n",
    "model.eval()\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx = text_to_token_ids(\"Every effor moves you\", tokenizer).to(inference_device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2f342ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e11a15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a96eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=len(probas))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5dc233e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "temperatures = [1, 0.1, 5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b1d4b02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c34bd501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f74dc5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3737b",
   "metadata": {},
   "source": [
    "## 2. Tok-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3c9dcd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500e+00, 6.2800e+00, 4.5100e+00])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c97fa706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100e+00,       -inf,       -inf, 6.7500e+00,       -inf,       -inf,\n",
      "              -inf, 6.2800e+00,       -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")),\n",
    "    other=next_token_logits\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "20f7df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100e+00,       -inf,       -inf, 6.7500e+00,       -inf,       -inf,\n",
      "              -inf, 6.2800e+00,       -inf])\n"
     ]
    }
   ],
   "source": [
    "# Alternate way\n",
    "\n",
    "# Create tensor containing -inf values\n",
    "new_logits = torch.full_like(next_token_logits, -torch.inf)\n",
    "new_logits[top_pos] = next_token_logits[top_pos]\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "993f5e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.1485e-02, 0.0000e+00, 0.0000e+00, 5.7755e-01, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 3.6097e-01, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f710c14a",
   "metadata": {},
   "source": [
    "## 3. Combining Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9d9d6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        \n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1) \n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a264ac11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as terr what one of the axi,\" he that with a curious.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(inference_device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eb189f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Loading and Saving model weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d86c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "402ae9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fb49f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),},\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19f6d882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5041e892",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Loading Pre-Trained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10303ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 01:55:52.109585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765743952.161168    4742 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765743952.177845    4742 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765743952.298138    4742 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765743952.298172    4742 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765743952.298173    4742 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765743952.298175    4742 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 01:55:52.313013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "537b1f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 01:56:06.399202: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "100ec3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2771fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape Mismatch\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce94411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.weight = assign(gpt.final_norm.weight, params[\"g\"])\n",
    "    gpt.final_norm.bias   = assign(gpt.final_norm.bias, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c07309e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
      "\n",
      "This would remove you from a battle\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9c68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76d32eac",
   "metadata": {},
   "source": [
    "Output text:\n",
    " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
    "\n",
    "This would remove you from a battle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
